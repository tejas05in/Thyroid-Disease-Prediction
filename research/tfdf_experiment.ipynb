{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_decision_forests as tfdf\n",
    "import pandas as pd\n",
    "from wurlitzer import sys_pipes\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"age\",\"sex\",\"on_thyroxine\",\"query_on_thyroxine\",\"on_antihyroid_meds\",\"sick\",\"pregnant\",\"thyroid_surgery\",\"I131_treatment\",\"query_hypothyroid\",\"query_hyperthyroid\",\"lithium\",\"goitre\",\"tumor\",\"hypopituitary\",\"psych\",\"TSH_measured\",\"TSH\",\"T3_measured\",\"T3\",\"TT4_measured\",\"TT4\",\"T4U_measured\",\"T4U\",\"FTI_measured\",\"FTI\",\"TBG_measured\",\"TBG\",\"referral_source\",\"target\"]\n",
    "df = pd.read_csv('/config/workspace/thyroid0387.data',names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tidy the target column\n",
    "df['patient_id'] = df[\"target\"].apply(lambda x: x.split(\"[\")[1].strip(']'))\n",
    "df['target'] = df[\"target\"].apply(lambda x: x.split(\"[\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing ? with np.nan\n",
    "df.replace({\"?\":np.nan},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting object to float\n",
    "num_cols = [\"TSH\",\"T3\",\"TT4\",\"T4U\",\"FTI\",\"TBG\"]\n",
    "for i in num_cols:\n",
    "    df[i] = df[i].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age cannot be 65526 \n",
    "# capping age to 100 years\n",
    "df = df[df[\"age\"] <= 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>on_thyroxine</th>\n",
       "      <th>query_on_thyroxine</th>\n",
       "      <th>on_antihyroid_meds</th>\n",
       "      <th>sick</th>\n",
       "      <th>pregnant</th>\n",
       "      <th>thyroid_surgery</th>\n",
       "      <th>I131_treatment</th>\n",
       "      <th>query_hypothyroid</th>\n",
       "      <th>...</th>\n",
       "      <th>TT4</th>\n",
       "      <th>T4U_measured</th>\n",
       "      <th>T4U</th>\n",
       "      <th>FTI_measured</th>\n",
       "      <th>FTI</th>\n",
       "      <th>TBG_measured</th>\n",
       "      <th>TBG</th>\n",
       "      <th>referral_source</th>\n",
       "      <th>target</th>\n",
       "      <th>patient_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>other</td>\n",
       "      <td>-</td>\n",
       "      <td>840801013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>128.0</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>other</td>\n",
       "      <td>-</td>\n",
       "      <td>840801014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t</td>\n",
       "      <td>11.0</td>\n",
       "      <td>other</td>\n",
       "      <td>-</td>\n",
       "      <td>840801042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t</td>\n",
       "      <td>26.0</td>\n",
       "      <td>other</td>\n",
       "      <td>-</td>\n",
       "      <td>840803046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t</td>\n",
       "      <td>36.0</td>\n",
       "      <td>other</td>\n",
       "      <td>S</td>\n",
       "      <td>840803047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age sex on_thyroxine query_on_thyroxine on_antihyroid_meds sick pregnant  \\\n",
       "0   29   F            f                  f                  f    f        f   \n",
       "1   29   F            f                  f                  f    f        f   \n",
       "2   41   F            f                  f                  f    f        f   \n",
       "3   36   F            f                  f                  f    f        f   \n",
       "4   32   F            f                  f                  f    f        f   \n",
       "\n",
       "  thyroid_surgery I131_treatment query_hypothyroid  ...    TT4 T4U_measured  \\\n",
       "0               f              f                 t  ...    NaN            f   \n",
       "1               f              f                 f  ...  128.0            f   \n",
       "2               f              f                 f  ...    NaN            f   \n",
       "3               f              f                 f  ...    NaN            f   \n",
       "4               f              f                 f  ...    NaN            f   \n",
       "\n",
       "  T4U FTI_measured FTI TBG_measured   TBG  referral_source target  patient_id  \n",
       "0 NaN            f NaN            f   NaN            other      -   840801013  \n",
       "1 NaN            f NaN            f   NaN            other      -   840801014  \n",
       "2 NaN            f NaN            t  11.0            other      -   840801042  \n",
       "3 NaN            f NaN            t  26.0            other      -   840803046  \n",
       "4 NaN            f NaN            t  36.0            other      S   840803047  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove reduntant columns\n",
    "df.drop(['TSH_measured','T3_measured','TT4_measured','T4U_measured','FTI_measured','TBG_measured','referral_source','patient_id','TBG'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a subset of target which can be classified as Hyper , hypo or Euthyroid (Negative) state\n",
    "df = df[df['target'].isin(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'AK', 'C|I', 'H|K', 'GK', 'FK', 'GI', 'GKJ', 'D|R', '-'])]\n",
    "# mapping the target column\n",
    "mapping = {'-':\"Negative\",\n",
    "           'A':'Hyperthyroid','AK':\"Hyperthyroid\",'B':\"Hyperthyroid\", 'C':\"Hyperthyroid\", 'C|I': 'Hyperthyroid', 'D':\"Hyperthyroid\", 'D|R':\"Hyperthyroid\",\n",
    "           'E': \"Hypothyroid\", 'F': \"Hypothyroid\", 'FK': \"Hypothyroid\", \"G\": \"Hypothyroid\", \"GK\": \"Hypothyroid\", \"GI\": \"Hypothyroid\", 'GKJ': 'Hypothyroid', 'H|K': 'Hypothyroid',\n",
    "          }\n",
    "df['target'] = df['target'].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset into a Pandas Dataframe.\n",
    "dataset_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label classes: ['Negative', 'Hypothyroid', 'Hyperthyroid']\n"
     ]
    }
   ],
   "source": [
    "# Name of the label column.\n",
    "label = \"target\"\n",
    "\n",
    "classes = dataset_df[label].unique().tolist()\n",
    "print(f\"Label classes: {classes}\")\n",
    "\n",
    "dataset_df[label] = dataset_df[label].map(classes.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7675\n",
      "5326 examples in training, 2349 examples for testing.\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into a training and a testing dataset.\n",
    "\n",
    "def split_dataset(dataset, test_ratio=0.30):\n",
    "  \"\"\"Splits a panda dataframe in two.\"\"\"\n",
    "  test_indices = np.random.rand(len(dataset)) < test_ratio\n",
    "  print(len(test_indices))\n",
    "  return dataset[~test_indices], dataset[test_indices]\n",
    "\n",
    "\n",
    "train_ds_pd, test_ds_pd = split_dataset(dataset_df)\n",
    "print(\"{} examples in training, {} examples for testing.\".format(\n",
    "    len(train_ds_pd), len(test_ds_pd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=label)\n",
    "test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_ds_pd, label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: The `num_threads` constructor argument is not set and the number of CPU is os.cpu_count()=64 > 32. Setting num_threads to 32. Set num_threads manually to use more than 32 cpus.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:The `num_threads` constructor argument is not set and the number of CPU is os.cpu_count()=64 > 32. Setting num_threads to 32. Set num_threads manually to use more than 32 cpus.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use /tmp/tmpkm6b1d_o as temporary training directory\n",
      "Reading training dataset...\n",
      "Training tensor examples:\n",
      "Features: {'age': <tf.Tensor 'data:0' shape=(None,) dtype=int64>, 'sex': <tf.Tensor 'data_1:0' shape=(None,) dtype=string>, 'on_thyroxine': <tf.Tensor 'data_2:0' shape=(None,) dtype=string>, 'query_on_thyroxine': <tf.Tensor 'data_3:0' shape=(None,) dtype=string>, 'on_antihyroid_meds': <tf.Tensor 'data_4:0' shape=(None,) dtype=string>, 'sick': <tf.Tensor 'data_5:0' shape=(None,) dtype=string>, 'pregnant': <tf.Tensor 'data_6:0' shape=(None,) dtype=string>, 'thyroid_surgery': <tf.Tensor 'data_7:0' shape=(None,) dtype=string>, 'I131_treatment': <tf.Tensor 'data_8:0' shape=(None,) dtype=string>, 'query_hypothyroid': <tf.Tensor 'data_9:0' shape=(None,) dtype=string>, 'query_hyperthyroid': <tf.Tensor 'data_10:0' shape=(None,) dtype=string>, 'lithium': <tf.Tensor 'data_11:0' shape=(None,) dtype=string>, 'goitre': <tf.Tensor 'data_12:0' shape=(None,) dtype=string>, 'tumor': <tf.Tensor 'data_13:0' shape=(None,) dtype=string>, 'hypopituitary': <tf.Tensor 'data_14:0' shape=(None,) dtype=string>, 'psych': <tf.Tensor 'data_15:0' shape=(None,) dtype=string>, 'TSH': <tf.Tensor 'data_16:0' shape=(None,) dtype=float64>, 'T3': <tf.Tensor 'data_17:0' shape=(None,) dtype=float64>, 'TT4': <tf.Tensor 'data_18:0' shape=(None,) dtype=float64>, 'T4U': <tf.Tensor 'data_19:0' shape=(None,) dtype=float64>, 'FTI': <tf.Tensor 'data_20:0' shape=(None,) dtype=float64>}\n",
      "Label: Tensor(\"data_21:0\", shape=(None,), dtype=int64)\n",
      "Weights: None\n",
      "Normalized tensor features:\n",
      " {'age': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast:0' shape=(None,) dtype=float32>), 'sex': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data_1:0' shape=(None,) dtype=string>), 'on_thyroxine': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data_2:0' shape=(None,) dtype=string>), 'query_on_thyroxine': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data_3:0' shape=(None,) dtype=string>), 'on_antihyroid_meds': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data_4:0' shape=(None,) dtype=string>), 'sick': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data_5:0' shape=(None,) dtype=string>), 'pregnant': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data_6:0' shape=(None,) dtype=string>), 'thyroid_surgery': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data_7:0' shape=(None,) dtype=string>), 'I131_treatment': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data_8:0' shape=(None,) dtype=string>), 'query_hypothyroid': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data_9:0' shape=(None,) dtype=string>), 'query_hyperthyroid': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data_10:0' shape=(None,) dtype=string>), 'lithium': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data_11:0' shape=(None,) dtype=string>), 'goitre': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data_12:0' shape=(None,) dtype=string>), 'tumor': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data_13:0' shape=(None,) dtype=string>), 'hypopituitary': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data_14:0' shape=(None,) dtype=string>), 'psych': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data_15:0' shape=(None,) dtype=string>), 'TSH': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_1:0' shape=(None,) dtype=float32>), 'T3': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_2:0' shape=(None,) dtype=float32>), 'TT4': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_3:0' shape=(None,) dtype=float32>), 'T4U': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_4:0' shape=(None,) dtype=float32>), 'FTI': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_5:0' shape=(None,) dtype=float32>)}\n",
      "Training dataset read in 0:00:00.446990. Found 5326 examples.\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 24-01-17 17:20:35.8482 IST kernel.cc:771] Start Yggdrasil model training\n",
      "[INFO 24-01-17 17:20:35.8483 IST kernel.cc:772] Collect training examples\n",
      "[INFO 24-01-17 17:20:35.8483 IST kernel.cc:785] Dataspec guide:\n",
      "column_guides {\n",
      "  column_name_pattern: \"^__LABEL$\"\n",
      "  type: CATEGORICAL\n",
      "  categorial {\n",
      "    min_vocab_frequency: 0\n",
      "    max_vocab_count: -1\n",
      "  }\n",
      "}\n",
      "default_column_guide {\n",
      "  categorial {\n",
      "    max_vocab_count: 2000\n",
      "  }\n",
      "  discretized_numerical {\n",
      "    maximum_num_bins: 255\n",
      "  }\n",
      "}\n",
      "ignore_columns_without_guides: false\n",
      "detect_numerical_as_discretized_numerical: false\n",
      "\n",
      "[INFO 24-01-17 17:20:35.8485 IST kernel.cc:391] Number of batches: 6\n",
      "[INFO 24-01-17 17:20:35.8485 IST kernel.cc:392] Number of examples: 5326\n",
      "[INFO 24-01-17 17:20:35.8553 IST kernel.cc:792] Training dataset:\n",
      "Number of records: 5326\n",
      "Number of columns: 22\n",
      "\n",
      "Number of columns by type:\n",
      "\tCATEGORICAL: 16 (72.7273%)\n",
      "\tNUMERICAL: 6 (27.2727%)\n",
      "\n",
      "Columns:\n",
      "\n",
      "CATEGORICAL: 16 (72.7273%)\n",
      "\t1: \"I131_treatment\" CATEGORICAL has-dict vocab-size:3 zero-ood-items most-frequent:\"f\" 5235 (98.2914%)\n",
      "\t6: \"__LABEL\" CATEGORICAL integerized vocab-size:4 no-ood-item\n",
      "\t8: \"goitre\" CATEGORICAL has-dict vocab-size:3 zero-ood-items most-frequent:\"f\" 5284 (99.2114%)\n",
      "\t9: \"hypopituitary\" CATEGORICAL has-dict vocab-size:2 zero-ood-items most-frequent:\"f\" 5326 (100%)\n",
      "\t10: \"lithium\" CATEGORICAL has-dict vocab-size:3 zero-ood-items most-frequent:\"f\" 5261 (98.7796%)\n",
      "\t11: \"on_antihyroid_meds\" CATEGORICAL has-dict vocab-size:3 zero-ood-items most-frequent:\"f\" 5260 (98.7608%)\n",
      "\t12: \"on_thyroxine\" CATEGORICAL has-dict vocab-size:3 zero-ood-items most-frequent:\"f\" 4716 (88.5468%)\n",
      "\t13: \"pregnant\" CATEGORICAL has-dict vocab-size:3 zero-ood-items most-frequent:\"f\" 5299 (99.4931%)\n",
      "\t14: \"psych\" CATEGORICAL has-dict vocab-size:3 zero-ood-items most-frequent:\"f\" 5052 (94.8554%)\n",
      "\t15: \"query_hyperthyroid\" CATEGORICAL has-dict vocab-size:3 zero-ood-items most-frequent:\"f\" 4930 (92.5648%)\n",
      "\t16: \"query_hypothyroid\" CATEGORICAL has-dict vocab-size:3 zero-ood-items most-frequent:\"f\" 4969 (93.297%)\n",
      "\t17: \"query_on_thyroxine\" CATEGORICAL has-dict vocab-size:3 zero-ood-items most-frequent:\"f\" 5247 (98.5167%)\n",
      "\t18: \"sex\" CATEGORICAL num-nas:180 (3.37965%) has-dict vocab-size:3 zero-ood-items most-frequent:\"F\" 3478 (67.5865%)\n",
      "\t19: \"sick\" CATEGORICAL has-dict vocab-size:3 zero-ood-items most-frequent:\"f\" 5122 (96.1697%)\n",
      "\t20: \"thyroid_surgery\" CATEGORICAL has-dict vocab-size:3 zero-ood-items most-frequent:\"f\" 5250 (98.573%)\n",
      "\t21: \"tumor\" CATEGORICAL has-dict vocab-size:3 zero-ood-items most-frequent:\"f\" 5189 (97.4277%)\n",
      "\n",
      "NUMERICAL: 6 (27.2727%)\n",
      "\t0: \"FTI\" NUMERICAL num-nas:474 (8.89974%) mean:110.746 min:1.4 max:839 sd:37.2599\n",
      "\t2: \"T3\" NUMERICAL num-nas:1506 (28.2764%) mean:2.01057 min:0.05 max:18 sd:0.827293\n",
      "\t3: \"T4U\" NUMERICAL num-nas:479 (8.99362%) mean:0.968815 min:0.19 max:2.12 sd:0.165656\n",
      "\t4: \"TSH\" NUMERICAL num-nas:503 (9.44424%) mean:5.92205 min:0.005 max:530 sd:28.3314\n",
      "\t5: \"TT4\" NUMERICAL num-nas:248 (4.6564%) mean:105.552 min:2 max:430 sd:32.8548\n",
      "\t7: \"age\" NUMERICAL mean:52.0445 min:1 max:97 sd:18.4901\n",
      "\n",
      "Terminology:\n",
      "\tnas: Number of non-available (i.e. missing) values.\n",
      "\tood: Out of dictionary.\n",
      "\tmanually-defined: Attribute whose type is manually defined by the user, i.e., the type was not automatically inferred.\n",
      "\ttokenized: The attribute value is obtained through tokenization.\n",
      "\thas-dict: The attribute is attached to a string dictionary e.g. a categorical attribute stored as a string.\n",
      "\tvocab-size: Number of unique values.\n",
      "\n",
      "[INFO 24-01-17 17:20:35.8553 IST kernel.cc:808] Configure learner\n",
      "[INFO 24-01-17 17:20:35.8557 IST kernel.cc:822] Training config:\n",
      "learner: \"HYPERPARAMETER_OPTIMIZER\"\n",
      "features: \"^FTI$\"\n",
      "features: \"^I131_treatment$\"\n",
      "features: \"^T3$\"\n",
      "features: \"^T4U$\"\n",
      "features: \"^TSH$\"\n",
      "features: \"^TT4$\"\n",
      "features: \"^age$\"\n",
      "features: \"^goitre$\"\n",
      "features: \"^hypopituitary$\"\n",
      "features: \"^lithium$\"\n",
      "features: \"^on_antihyroid_meds$\"\n",
      "features: \"^on_thyroxine$\"\n",
      "features: \"^pregnant$\"\n",
      "features: \"^psych$\"\n",
      "features: \"^query_hyperthyroid$\"\n",
      "features: \"^query_hypothyroid$\"\n",
      "features: \"^query_on_thyroxine$\"\n",
      "features: \"^sex$\"\n",
      "features: \"^sick$\"\n",
      "features: \"^thyroid_surgery$\"\n",
      "features: \"^tumor$\"\n",
      "label: \"^__LABEL$\"\n",
      "task: CLASSIFICATION\n",
      "metadata {\n",
      "  framework: \"TF Keras\"\n",
      "}\n",
      "[yggdrasil_decision_forests.model.hyperparameters_optimizer_v2.proto.hyperparameters_optimizer_config] {\n",
      "  base_learner {\n",
      "    learner: \"RANDOM_FOREST\"\n",
      "    features: \"^FTI$\"\n",
      "    features: \"^I131_treatment$\"\n",
      "    features: \"^T3$\"\n",
      "    features: \"^T4U$\"\n",
      "    features: \"^TSH$\"\n",
      "    features: \"^TT4$\"\n",
      "    features: \"^age$\"\n",
      "    features: \"^goitre$\"\n",
      "    features: \"^hypopituitary$\"\n",
      "    features: \"^lithium$\"\n",
      "    features: \"^on_antihyroid_meds$\"\n",
      "    features: \"^on_thyroxine$\"\n",
      "    features: \"^pregnant$\"\n",
      "    features: \"^psych$\"\n",
      "    features: \"^query_hyperthyroid$\"\n",
      "    features: \"^query_hypothyroid$\"\n",
      "    features: \"^query_on_thyroxine$\"\n",
      "    features: \"^sex$\"\n",
      "    features: \"^sick$\"\n",
      "    features: \"^thyroid_surgery$\"\n",
      "    features: \"^tumor$\"\n",
      "    label: \"^__LABEL$\"\n",
      "    task: CLASSIFICATION\n",
      "    random_seed: 42\n",
      "    pure_serving_model: false\n",
      "    [yggdrasil_decision_forests.model.random_forest.proto.random_forest_config] {\n",
      "      num_trees: 300\n",
      "      decision_tree {\n",
      "        max_depth: 16\n",
      "        min_examples: 5\n",
      "        in_split_min_examples_check: true\n",
      "        keep_non_leaf_label_distribution: true\n",
      "        num_candidate_attributes: 0\n",
      "        missing_value_policy: GLOBAL_IMPUTATION\n",
      "        allow_na_conditions: false\n",
      "        categorical_set_greedy_forward {\n",
      "          sampling: 0.1\n",
      "          max_num_items: -1\n",
      "          min_item_frequency: 1\n",
      "        }\n",
      "        growing_strategy_local {\n",
      "        }\n",
      "        categorical {\n",
      "          cart {\n",
      "          }\n",
      "        }\n",
      "        axis_aligned_split {\n",
      "        }\n",
      "        internal {\n",
      "          sorting_strategy: PRESORTED\n",
      "        }\n",
      "        uplift {\n",
      "          min_examples_in_treatment: 5\n",
      "          split_score: KULLBACK_LEIBLER\n",
      "        }\n",
      "      }\n",
      "      winner_take_all_inference: true\n",
      "      compute_oob_performances: true\n",
      "      compute_oob_variable_importances: false\n",
      "      num_oob_variable_importances_permutations: 1\n",
      "      bootstrap_training_dataset: true\n",
      "      bootstrap_size_ratio: 1\n",
      "      adapt_bootstrap_size_ratio_for_maximum_training_duration: false\n",
      "      sampling_with_replacement: true\n",
      "    }\n",
      "  }\n",
      "  optimizer {\n",
      "    optimizer_key: \"RANDOM\"\n",
      "    [yggdrasil_decision_forests.model.hyperparameters_optimizer_v2.proto.random] {\n",
      "      num_trials: 20\n",
      "    }\n",
      "  }\n",
      "  search_space {\n",
      "    fields {\n",
      "      name: \"max_depth\"\n",
      "      discrete_candidates {\n",
      "        possible_values {\n",
      "          integer: 4\n",
      "        }\n",
      "        possible_values {\n",
      "          integer: 5\n",
      "        }\n",
      "        possible_values {\n",
      "          integer: 6\n",
      "        }\n",
      "        possible_values {\n",
      "          integer: 7\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  base_learner_deployment {\n",
      "    num_threads: 1\n",
      "  }\n",
      "}\n",
      "\n",
      "[INFO 24-01-17 17:20:35.8559 IST kernel.cc:825] Deployment config:\n",
      "cache_path: \"/tmp/tmpkm6b1d_o/working_cache\"\n",
      "num_threads: 32\n",
      "try_resume_training: true\n",
      "\n",
      "[INFO 24-01-17 17:20:35.8560 IST kernel.cc:887] Train model\n",
      "[INFO 24-01-17 17:20:35.8563 IST hyperparameters_optimizer.cc:209] Hyperparameter search space:\n",
      "fields {\n",
      "  name: \"max_depth\"\n",
      "  discrete_candidates {\n",
      "    possible_values {\n",
      "      integer: 4\n",
      "    }\n",
      "    possible_values {\n",
      "      integer: 5\n",
      "    }\n",
      "    possible_values {\n",
      "      integer: 6\n",
      "    }\n",
      "    possible_values {\n",
      "      integer: 7\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "[INFO 24-01-17 17:20:35.8563 IST hyperparameters_optimizer.cc:500] Start local tuner with 32 thread(s)\n",
      "[INFO 24-01-17 17:20:35.8599 IST random_forest.cc:[INFO416] Training random forest on [INFO5326 example(s) and 21 feature(s).\n",
      " 24-01-17 17:20:35.8599 IST random_forest.cc:416] Training random forest on 5326 example(s) and 21 feature(s).\n",
      " 24-01-17 17:20:35.8599 IST random_forest.cc:416] Training random forest on 5326 example(s) and 21 feature(s).\n",
      "[INFO 24-01-17 17:20:35.8600 IST random_forest.cc:416] Training random forest on 5326 example(s) and 21 feature(s).\n",
      "[INFO 24-01-17 17:20:35.8683 IST random_forest.cc:802] Training of tree  1/300 (tree index:0) done accuracy:0.915784 logloss:3.03545\n",
      "[INFO 24-01-17 17:20:35.8686 IST random_forest.cc:802] Training of tree  1/300 (tree index:0) done accuracy:0.982854 logloss:0.617995\n",
      "[INFO 24-01-17 17:20:35.8689 IST random_forest.cc:802] Training of tree  1/300 (tree index:0) done accuracy:0.931417 logloss:2.47198\n",
      "[INFO 24-01-17 17:20:35.8701 IST random_forest.cc:802] Training of tree  1/300 (tree index:0) done accuracy:0.98235 logloss:0.636171\n",
      "[INFO 24-01-17 17:20:35.8902 IST random_forest.cc:802] Training of tree  11/300 (tree index:10) done accuracy:0.965426 logloss:0.434886\n",
      "[INFO 24-01-17 17:20:35.8975 IST random_forest.cc:802] Training of tree  11/300 (tree index:10) done accuracy:0.974495 logloss:0.412052\n",
      "[INFO 24-01-17 17:20:35.9050 IST random_forest.cc:802] Training of tree  11/300 (tree index:10) done accuracy:0.970716 logloss:0.331929\n",
      "[INFO 24-01-17 17:20:35.9113 IST random_forest.cc:802] Training of tree  21/300 (tree index:20) done accuracy:0.962817 logloss:0.267385\n",
      "[INFO 24-01-17 17:20:35.9146 IST random_forest.cc:802] Training of tree  11/300 (tree index:10) done accuracy:0.981107 logloss:0.263055\n",
      "[INFO 24-01-17 17:20:35.9263 IST random_forest.cc:802] Training of tree  21/300 (tree index:20) done accuracy:0.979343 logloss:0.234751\n",
      "[INFO 24-01-17 17:20:35.9329 IST random_forest.cc:802] Training of tree  31/300 (tree index:30) done accuracy:0.976155 logloss:0.208465\n",
      "[INFO 24-01-17 17:20:35.9442 IST random_forest.cc:802] Training of tree  21/300 (tree index:20) done accuracy:0.977089 logloss:0.203651\n",
      "[INFO 24-01-17 17:20:35.9553 IST random_forest.cc:802] Training of tree  41/300 (tree index:40) done accuracy:0.976906 logloss:0.185451\n",
      "[INFO 24-01-17 17:20:35.9561 IST random_forest.cc:802] Training of tree  31/300 (tree index:30) done accuracy:0.980849 logloss:0.133137\n",
      "[INFO 24-01-17 17:20:35.9596 IST random_forest.cc:802] Training of tree  21/300 (tree index:20) done accuracy:0.983099 logloss:0.174268\n",
      "[INFO 24-01-17 17:20:35.9771 IST random_forest.cc:802] Training of tree  51/300 (tree index:50) done accuracy:0.975967 logloss:0.168831\n",
      "[INFO 24-01-17 17:20:35.9803 IST random_forest.cc:802] Training of tree  31/300 (tree index:30) done accuracy:0.981036 logloss:0.174723\n",
      "[INFO 24-01-17 17:20:35.9859 IST random_forest.cc:802] Training of tree  41/300 (tree index:40) done accuracy:0.981412 logloss:0.107323\n",
      "[INFO 24-01-17 17:20:35.9983 IST random_forest.cc:802] Training of tree  61/300 (tree index:60) done accuracy:0.974653 logloss:0.153623\n",
      "[INFO 24-01-17 17:20:36.0039 IST random_forest.cc:802] Training of tree  31/300 (tree index:30) done accuracy:0.98329 logloss:0.142753\n",
      "[INFO 24-01-17 17:20:36.0156 IST random_forest.cc:802] Training of tree  51/300 (tree index:50) done accuracy:0.982163 logloss:0.101848\n",
      "[INFO 24-01-17 17:20:36.0167 IST random_forest.cc:802] Training of tree  41/300 (tree index:40) done accuracy:0.981975 logloss:0.142664\n",
      "[INFO 24-01-17 17:20:36.0199 IST random_forest.cc:802] Training of tree  71/300 (tree index:70) done accuracy:0.977281 logloss:0.154043\n",
      "[INFO 24-01-17 17:20:36.0415 IST random_forest.cc:802] Training of tree  81/300 (tree index:80) done accuracy:0.978783 logloss:0.140355\n",
      "[INFO 24-01-17 17:20:36.0439 IST random_forest.cc:802] Training of tree  61/300 (tree index:60) done accuracy:0.981975 logloss:0.0889231\n",
      "[INFO 24-01-17 17:20:36.0520 IST random_forest.cc:802] Training of tree  41/300 (tree index:40) done accuracy:0.983853 logloss:0.105165\n",
      "[INFO 24-01-17 17:20:36.0558 IST random_forest.cc:802] Training of tree  51/300 (tree index:50) done accuracy:0.982351 logloss:0.128996\n",
      "[INFO 24-01-17 17:20:36.0630 IST random_forest.cc:802] Training of tree  91/300 (tree index:90) done accuracy:0.978596 logloss:0.128522\n",
      "[INFO 24-01-17 17:20:36.0737 IST random_forest.cc:802] Training of tree  71/300 (tree index:70) done accuracy:0.982726 logloss:0.0879681\n",
      "[INFO 24-01-17 17:20:36.0854 IST random_forest.cc:802] Training of tree  101/300 (tree index:100) done accuracy:0.980098 logloss:0.128545\n",
      "[INFO 24-01-17 17:20:36.0926 IST random_forest.cc:802] Training of tree  61/300 (tree index:60) done accuracy:0.982538 logloss:0.0958432\n",
      "[INFO 24-01-17 17:20:36.0989 IST random_forest.cc:802] Training of tree  51/300 (tree index:50) done accuracy:0.983853 logloss:0.0868575\n",
      "[INFO 24-01-17 17:20:36.1033 IST random_forest.cc:802] Training of tree  81/300 (tree index:80) done accuracy:0.982538 logloss:0.0895546\n",
      "[INFO 24-01-17 17:20:36.1074 IST random_forest.cc:802] Training of tree  111/300 (tree index:110) done accuracy:0.980661 logloss:0.127966\n",
      "[INFO 24-01-17 17:20:36.1293 IST random_forest.cc:802] Training of tree  121/300 (tree index:120) done accuracy:0.980285 logloss:0.115186\n",
      "[INFO 24-01-17 17:20:36.1297 IST random_forest.cc:802] Training of tree  71/300 (tree index:70) done accuracy:0.982726 logloss:0.0825373\n",
      "[INFO 24-01-17 17:20:36.1338 IST random_forest.cc:802] Training of tree  91/300 (tree index:90) done accuracy:0.982163 logloss:0.0901951\n",
      "[INFO 24-01-17 17:20:36.1451 IST random_forest.cc:802] Training of tree  61/300 (tree index:60) done accuracy:0.984041 logloss:0.087301\n",
      "[INFO 24-01-17 17:20:36.1510 IST random_forest.cc:802] Training of tree  131/300 (tree index:130) done accuracy:0.980661 logloss:0.10971\n",
      "[INFO 24-01-17 17:20:36.1646 IST random_forest.cc:802] Training of tree  101/300 (tree index:100) done accuracy:0.981975 logloss:0.0827526\n",
      "[INFO 24-01-17 17:20:36.1683 IST random_forest.cc:802] Training of tree  81/300 (tree index:80) done accuracy:0.983102 logloss:0.0693964\n",
      "[INFO 24-01-17 17:20:36.1728 IST random_forest.cc:802] Training of tree  141/300 (tree index:140) done accuracy:0.97991 logloss:0.109912\n",
      "[INFO 24-01-17 17:20:36.1890 IST random_forest.cc:802] Training of tree  71/300 (tree index:70) done accuracy:0.984041 logloss:0.0751795\n",
      "[INFO 24-01-17 17:20:36.1946 IST random_forest.cc:802] Training of tree  151/300 (tree index:150) done accuracy:0.980661 logloss:0.101463\n",
      "[INFO 24-01-17 17:20:36.1952 IST random_forest.cc:802] Training of tree  111/300 (tree index:110) done accuracy:0.982538 logloss:0.0752865\n",
      "[INFO 24-01-17 17:20:36.2051 IST random_forest.cc:802] Training of tree  91/300 (tree index:90) done accuracy:0.98329 logloss:0.056365\n",
      "[INFO 24-01-17 17:20:36.2172 IST random_forest.cc:802] Training of tree  161/300 (tree index:160) done accuracy:0.980849 logloss:0.100112\n",
      "[INFO 24-01-17 17:20:36.2250 IST random_forest.cc:802] Training of tree  121/300 (tree index:120) done accuracy:0.982163 logloss:0.0750538\n",
      "[INFO 24-01-17 17:20:36.2333 IST random_forest.cc:802] Training of tree  81/300 (tree index:80) done accuracy:0.983853 logloss:0.0617437\n",
      "[INFO 24-01-17 17:20:36.2395 IST random_forest.cc:802] Training of tree  171/300 (tree index:170) done accuracy:0.980849 logloss:0.100702\n",
      "[INFO 24-01-17 17:20:36.2437 IST random_forest.cc:802] Training of tree  101/300 (tree index:100) done accuracy:0.983477 logloss:0.0553252\n",
      "[INFO 24-01-17 17:20:36.2547 IST random_forest.cc:802] Training of tree  131/300 (tree index:130) done accuracy:0.982726 logloss:0.0756693\n",
      "[INFO 24-01-17 17:20:36.2610 IST random_forest.cc:802] Training of tree  181/300 (tree index:180) done accuracy:0.980849 logloss:0.101114\n",
      "[INFO 24-01-17 17:20:36.2795 IST random_forest.cc:802] Training of tree  111/300 (tree index:110) done accuracy:0.98329 logloss:0.0549028\n",
      "[INFO 24-01-17 17:20:36.2817 IST random_forest.cc:802] Training of tree  91/300 (tree index:90) done accuracy:0.983477 logloss:0.0621338\n",
      "[INFO 24-01-17 17:20:36.2827 IST random_forest.cc:802] Training of tree  191/300 (tree index:190) done accuracy:0.981412 logloss:0.100347\n",
      "[INFO 24-01-17 17:20:36.2837 IST random_forest.cc:802] Training of tree  141/300 (tree index:140) done accuracy:0.983102 logloss:0.0759777\n",
      "[INFO 24-01-17 17:20:36.3049 IST random_forest.cc:802] Training of tree  201/300 (tree index:200) done accuracy:0.9816 logloss:0.0998519\n",
      "[INFO 24-01-17 17:20:36.3126 IST random_forest.cc:802] Training of tree  151/300 (tree index:150) done accuracy:0.983102 logloss:0.0693547\n",
      "[INFO 24-01-17 17:20:36.3169 IST random_forest.cc:802] Training of tree  121/300 (tree index:120) done accuracy:0.98329 logloss:0.0551525\n",
      "[INFO 24-01-17 17:20:36.3257 IST random_forest.cc:802] Training of tree  211/300 (tree index:210) done accuracy:0.981224 logloss:0.0871029\n",
      "[INFO 24-01-17 17:20:36.3280 IST random_forest.cc:802] Training of tree  101/300 (tree index:100) done accuracy:0.983477 logloss:0.0559609\n",
      "[INFO 24-01-17 17:20:36.3409 IST random_forest.cc:802] Training of tree  161/300 (tree index:160) done accuracy:0.982914 logloss:0.0695688\n",
      "[INFO 24-01-17 17:20:36.3470 IST random_forest.cc:802] Training of tree  221/300 (tree index:220) done accuracy:0.981412 logloss:0.0868531\n",
      "[INFO 24-01-17 17:20:36.3545 IST random_forest.cc:802] Training of tree  131/300 (tree index:130) done accuracy:0.983477 logloss:0.0560159\n",
      "[INFO 24-01-17 17:20:36.3685 IST random_forest.cc:802] Training of tree  231/300 (tree index:230) done accuracy:0.981412 logloss:0.0870756\n",
      "[INFO 24-01-17 17:20:36.3718 IST random_forest.cc:802] Training of tree  171/300 (tree index:170) done accuracy:0.98329 logloss:0.0697208\n",
      "[INFO 24-01-17 17:20:36.3759 IST random_forest.cc:802] Training of tree  111/300 (tree index:110) done accuracy:0.983665 logloss:0.0556003\n",
      "[INFO 24-01-17 17:20:36.3913 IST random_forest.cc:802] Training of tree  141/300 (tree index:140) done accuracy:0.983102 logloss:0.0554408\n",
      "[INFO 24-01-17 17:20:36.3913 IST random_forest.cc:802] Training of tree  241/300 (tree index:240) done accuracy:0.981412 logloss:0.0864911\n",
      "[INFO 24-01-17 17:20:36.4016 IST random_forest.cc:802] Training of tree  181/300 (tree index:180) done accuracy:0.98329 logloss:0.0694957\n",
      "[INFO 24-01-17 17:20:36.4136 IST random_forest.cc:802] Training of tree  251/300 (tree index:250) done accuracy:0.981412 logloss:0.0868575\n",
      "[INFO 24-01-17 17:20:36.4240 IST random_forest.cc:802] Training of tree  121/300 (tree index:120) done accuracy:0.983477 logloss:0.05555\n",
      "[INFO 24-01-17 17:20:36.4284 IST random_forest.cc:802] Training of tree  151/300 (tree index:150) done accuracy:0.982914 logloss:0.0552008\n",
      "[INFO 24-01-17 17:20:36.4303 IST random_forest.cc:802] Training of tree  191/300 (tree index:190) done accuracy:0.982914 logloss:0.0698747\n",
      "[INFO 24-01-17 17:20:36.4353 IST random_forest.cc:802] Training of tree  261/300 (tree index:260) done accuracy:0.981412 logloss:0.0808717\n",
      "[INFO 24-01-17 17:20:36.4572 IST random_forest.cc:802] Training of tree  271/300 (tree index:270) done accuracy:0.981412 logloss:0.08014\n",
      "[INFO 24-01-17 17:20:36.4599 IST random_forest.cc:802] Training of tree  201/300 (tree index:200) done accuracy:0.983477 logloss:0.069337\n",
      "[INFO 24-01-17 17:20:36.4652 IST random_forest.cc:802] Training of tree  161/300 (tree index:160) done accuracy:0.983102 logloss:0.0542888\n",
      "[INFO 24-01-17 17:20:36.4709 IST random_forest.cc:802] Training of tree  131/300 (tree index:130) done accuracy:0.983665 logloss:0.0553806\n",
      "[INFO 24-01-17 17:20:36.4802 IST random_forest.cc:802] Training of tree  281/300 (tree index:280) done accuracy:0.981412 logloss:0.0792513\n",
      "[INFO 24-01-17 17:20:36.4882 IST random_forest.cc:802] Training of tree  211/300 (tree index:210) done accuracy:0.982914 logloss:0.0696946\n",
      "[INFO 24-01-17 17:20:36.5018 IST random_forest.cc:802] Training of tree  291/300 (tree index:290) done accuracy:0.981412 logloss:0.0797643\n",
      "[INFO 24-01-17 17:20:36.5027 IST random_forest.cc:802] Training of tree  171/300 (tree index:170) done accuracy:0.98329 logloss:0.0479703\n",
      "[INFO 24-01-17 17:20:36.5176 IST random_forest.cc:802] Training of tree  221/300 (tree index:220) done accuracy:0.982914 logloss:0.0696414\n",
      "[INFO 24-01-17 17:20:36.5209 IST random_forest.cc:802] Training of tree  300/300 (tree index:299) done accuracy:0.981787 logloss:0.0803849\n",
      "[INFO 24-01-17 17:20:36.5210 IST random_forest.cc:882] Final OOB metrics: accuracy:0.981787 logloss:0.0803849\n",
      "[INFO 24-01-17 17:20:36.5221 IST hyperparameters_optimizer.cc:582] [1/20] Score: 0.981787 / 0.981787 HParams: fields { name: \"max_depth\" value { integer: 4 } }\n",
      "[INFO 24-01-17 17:20:36.5229 IST random_forest.cc:802] Training of tree  141/300 (tree index:140) done accuracy:0.984228 logloss:0.0554933\n",
      "[INFO 24-01-17 17:20:36.5414 IST random_forest.cc:802] Training of tree  181/300 (tree index:180) done accuracy:0.98329 logloss:0.0485193\n",
      "[INFO 24-01-17 17:20:36.5464 IST random_forest.cc:802] Training of tree  231/300 (tree index:230) done accuracy:0.98329 logloss:0.069377\n",
      "[INFO 24-01-17 17:20:36.5714 IST random_forest.cc:802] Training of tree  151/300 (tree index:150) done accuracy:0.983665 logloss:0.0561596\n",
      "[INFO 24-01-17 17:20:36.5772 IST random_forest.cc:802] Training of tree  241/300 (tree index:240) done accuracy:0.983102 logloss:0.069705\n",
      "[INFO 24-01-17 17:20:36.5790 IST random_forest.cc:802] Training of tree  191/300 (tree index:190) done accuracy:0.983477 logloss:0.0480306\n",
      "[INFO 24-01-17 17:20:36.6067 IST random_forest.cc:802] Training of tree  251/300 (tree index:250) done accuracy:0.98329 logloss:0.0707624\n",
      "[INFO 24-01-17 17:20:36.6177 IST random_forest.cc:802] Training of tree  201/300 (tree index:200) done accuracy:0.983477 logloss:0.0478311\n",
      "[INFO 24-01-17 17:20:36.6183 IST random_forest.cc:802] Training of tree  161/300 (tree index:160) done accuracy:0.983665 logloss:0.0558372\n",
      "[INFO 24-01-17 17:20:36.6365 IST random_forest.cc:802] Training of tree  261/300 (tree index:260) done accuracy:0.98329 logloss:0.0650136\n",
      "[INFO 24-01-17 17:20:36.6555 IST random_forest.cc:802] Training of tree  211/300 (tree index:210) done accuracy:0.98329 logloss:0.0478031\n",
      "[INFO 24-01-17 17:20:36.6659 IST random_forest.cc:802] Training of tree  271/300 (tree index:270) done accuracy:0.98329 logloss:0.065123\n",
      "[INFO 24-01-17 17:20:36.6662 IST random_forest.cc:802] Training of tree  171/300 (tree index:170) done accuracy:0.983665 logloss:0.055673\n",
      "[INFO 24-01-17 17:20:36.6919 IST random_forest.cc:802] Training of tree  221/300 (tree index:220) done accuracy:0.98329 logloss:0.0476633\n",
      "[INFO 24-01-17 17:20:36.6957 IST random_forest.cc:802] Training of tree  281/300 (tree index:280) done accuracy:0.98329 logloss:0.0651749\n",
      "[INFO 24-01-17 17:20:36.7130 IST random_forest.cc:802] Training of tree  181/300 (tree index:180) done accuracy:0.983853 logloss:0.0559052\n",
      "[INFO 24-01-17 17:20:36.7253 IST random_forest.cc:802] Training of tree  291/300 (tree index:290) done accuracy:0.98329 logloss:0.0650557\n",
      "[INFO 24-01-17 17:20:36.7278 IST random_forest.cc:802] Training of tree  231/300 (tree index:230) done accuracy:0.983477 logloss:0.0477895\n",
      "[INFO 24-01-17 17:20:36.7508 IST random_forest.cc:802] Training of tree  300/300 (tree index:299) done accuracy:0.98329 logloss:0.0651697\n",
      "[INFO 24-01-17 17:20:36.7509 IST random_forest.cc:882] Final OOB metrics: accuracy:0.98329 logloss:0.0651697\n",
      "[INFO 24-01-17 17:20:36.7540 IST hyperparameters_optimizer.cc:582] [2/20] Score: 0.98329 / 0.98329 HParams: fields { name: \"max_depth\" value { integer: 5 } }\n",
      "[INFO 24-01-17 17:20:36.7609 IST random_forest.cc:802] Training of tree  191/300 (tree index:190) done accuracy:0.983853 logloss:0.0557867\n",
      "[INFO 24-01-17 17:20:36.7660 IST random_forest.cc:802] Training of tree  241/300 (tree index:240) done accuracy:0.983477 logloss:0.0480903\n",
      "[INFO 24-01-17 17:20:36.8033 IST random_forest.cc:802] Training of tree  251/300 (tree index:250) done accuracy:0.98329 logloss:0.0480068\n",
      "[INFO 24-01-17 17:20:36.8063 IST random_forest.cc:802] Training of tree  201/300 (tree index:200) done accuracy:0.983665 logloss:0.0554635\n",
      "[INFO 24-01-17 17:20:36.8401 IST random_forest.cc:802] Training of tree  261/300 (tree index:260) done accuracy:0.983102 logloss:0.0480053\n",
      "[INFO 24-01-17 17:20:36.8532 IST random_forest.cc:802] Training of tree  211/300 (tree index:210) done accuracy:0.983477 logloss:0.0554036\n",
      "[INFO 24-01-17 17:20:36.8797 IST random_forest.cc:802] Training of tree  271/300 (tree index:270) done accuracy:0.983102 logloss:0.0482978\n",
      "[INFO 24-01-17 17:20:36.9019 IST random_forest.cc:802] Training of tree  221/300 (tree index:220) done accuracy:0.983853 logloss:0.0551438\n",
      "[INFO 24-01-17 17:20:36.9153 IST random_forest.cc:802] Training of tree  281/300 (tree index:280) done accuracy:0.98329 logloss:0.0480143\n",
      "[INFO 24-01-17 17:20:36.9494 IST random_forest.cc:802] Training of tree  231/300 (tree index:230) done accuracy:0.983853 logloss:0.0550737\n",
      "[INFO 24-01-17 17:20:36.9521 IST random_forest.cc:802] Training of tree  291/300 (tree index:290) done accuracy:0.98329 logloss:0.0480069\n",
      "[INFO 24-01-17 17:20:36.9849 IST random_forest.cc:802] Training of tree  300/300 (tree index:299) done accuracy:0.98329 logloss:0.0483401\n",
      "[INFO 24-01-17 17:20:36.9850 IST random_forest.cc:882] Final OOB metrics: accuracy:0.98329 logloss:0.0483401\n",
      "[INFO 24-01-17 17:20:36.9890 IST hyperparameters_optimizer.cc:582] [3/20] Score: 0.98329 / 0.98329 HParams: fields { name: \"max_depth\" value { integer: 6 } }\n",
      "[INFO 24-01-17 17:20:36.9975 IST random_forest.cc:802] Training of tree  241/300 (tree index:240) done accuracy:0.983853 logloss:0.0490497\n",
      "[INFO 24-01-17 17:20:37.0447 IST random_forest.cc:802] Training of tree  251/300 (tree index:250) done accuracy:0.983665 logloss:0.0490929\n",
      "[INFO 24-01-17 17:20:37.0919 IST random_forest.cc:802] Training of tree  261/300 (tree index:260) done accuracy:0.983477 logloss:0.0490362\n",
      "[INFO 24-01-17 17:20:37.1391 IST random_forest.cc:802] Training of tree  271/300 (tree index:270) done accuracy:0.983477 logloss:0.0487961\n",
      "[INFO 24-01-17 17:20:37.1851 IST random_forest.cc:802] Training of tree  281/300 (tree index:280) done accuracy:0.98329 logloss:0.0485993\n",
      "[INFO 24-01-17 17:20:37.2313 IST random_forest.cc:802] Training of tree  291/300 (tree index:290) done accuracy:0.983665 logloss:0.0484443\n",
      "[INFO 24-01-17 17:20:37.2730 IST random_forest.cc:802] Training of tree  300/300 (tree index:299) done accuracy:0.983665 logloss:0.048643\n",
      "[INFO 24-01-17 17:20:37.2731 IST random_forest.cc:882] Final OOB metrics: accuracy:0.983665 logloss:0.048643\n",
      "[INFO 24-01-17 17:20:37.2835 IST hyperparameters_optimizer.cc:582] [4/20] Score: 0.983665 / 0.983665 HParams: fields { name: \"max_depth\" value { integer: 7 } }\n",
      "[INFO 24-01-17 17:20:37.2862 IST hyperparameters_optimizer.cc:219] Best hyperparameters:\n",
      "fields {\n",
      "  name: \"max_depth\"\n",
      "  value {\n",
      "    integer: 7\n",
      "  }\n",
      "}\n",
      "\n",
      "[INFO 24-01-17 17:20:37.2863 IST kernel.cc:919] Export model in log directory: /tmp/tmpkm6b1d_o with prefix 38a3e78bc43448a2\n",
      "[INFO 24-01-17 17:20:37.3076 IST kernel.cc:937] Save model in resources\n",
      "[INFO 24-01-17 17:20:37.3104 IST abstract_model.cc:881] Model self evaluation:\n",
      "Number of predictions (without weights): 5326\n",
      "Number of predictions (with weights): 5326\n",
      "Task: CLASSIFICATION\n",
      "Label: __LABEL\n",
      "\n",
      "Accuracy: 0.983665  CI95[W][0.980509 0.986417]\n",
      "LogLoss: : 0.048643\n",
      "ErrorRate: : 0.016335\n",
      "\n",
      "Default Accuracy: : 0.879084\n",
      "Default LogLoss: : 0.436535\n",
      "Default ErrorRate: : 0.120916\n",
      "\n",
      "Confusion Table:\n",
      "truth\\prediction\n",
      "      1    2    3\n",
      "1  4648   17   17\n",
      "2    10  474    0\n",
      "3    43    0  117\n",
      "Total: 5326\n",
      "\n",
      "\n",
      "[INFO 24-01-17 17:20:37.3463 IST kernel.cc:1233] Loading model from path /tmp/tmpkm6b1d_o/model/ with prefix 38a3e78bc43448a2\n",
      "[INFO 24-01-17 17:20:37.4056 IST decision_forest.cc:660] Model loaded with 300 root(s), 18022 node(s), and 18 input feature(s).\n",
      "[INFO 24-01-17 17:20:37.4056 IST abstract_model.cc:1344] Engine \"RandomForestGeneric\" built\n",
      "[INFO 24-01-17 17:20:37.4057 IST kernel.cc:1061] Use fast generic engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained in 0:00:01.568050\n",
      "Compiling model...\n",
      "Model compiled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f6ffc601550>"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner = tfdf.tuner.RandomSearch(num_trials=20)\n",
    "\n",
    "# Hyper-parameters to optimize.\n",
    "tuner.choice(\"max_depth\", [4, 5, 6, 7])\n",
    "\n",
    "# Specify the model.\n",
    "model_1 = tfdf.keras.RandomForestModel(verbose=2,tuner=tuner,random_seed = 42)\n",
    "\n",
    "# Train the model.\n",
    "model_1.fit(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - accuracy: 0.9851\n",
      "\n",
      "loss: 0.0000\n",
      "accuracy: 0.9851\n"
     ]
    }
   ],
   "source": [
    "model_1.compile(metrics=[\"accuracy\"])\n",
    "evaluation = model_1.evaluate(test_ds, return_dict=True)\n",
    "print()\n",
    "\n",
    "for name, value in evaluation.items():\n",
    "  print(f\"{name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"random_forest_model_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      "=================================================================\n",
      "Total params: 1 (1.00 Byte)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 1 (1.00 Byte)\n",
      "_________________________________________________________________\n",
      "Type: \"RANDOM_FOREST\"\n",
      "Task: CLASSIFICATION\n",
      "Label: \"__LABEL\"\n",
      "\n",
      "Input Features (21):\n",
      "\tFTI\n",
      "\tI131_treatment\n",
      "\tT3\n",
      "\tT4U\n",
      "\tTSH\n",
      "\tTT4\n",
      "\tage\n",
      "\tgoitre\n",
      "\thypopituitary\n",
      "\tlithium\n",
      "\ton_antihyroid_meds\n",
      "\ton_thyroxine\n",
      "\tpregnant\n",
      "\tpsych\n",
      "\tquery_hyperthyroid\n",
      "\tquery_hypothyroid\n",
      "\tquery_on_thyroxine\n",
      "\tsex\n",
      "\tsick\n",
      "\tthyroid_surgery\n",
      "\ttumor\n",
      "\n",
      "No weights\n",
      "\n",
      "Variable Importance: INV_MEAN_MIN_DEPTH:\n",
      "    1.                \"TSH\"  0.323719 ################\n",
      "    2.                \"FTI\"  0.288116 ############\n",
      "    3.                \"TT4\"  0.278822 ###########\n",
      "    4.                 \"T3\"  0.228731 ######\n",
      "    5.                \"T4U\"  0.188294 ##\n",
      "    6.       \"on_thyroxine\"  0.179285 #\n",
      "    7. \"query_hyperthyroid\"  0.178831 #\n",
      "    8.  \"query_hypothyroid\"  0.169052 \n",
      "    9.                \"age\"  0.168358 \n",
      "   10.              \"tumor\"  0.167018 \n",
      "   11.              \"psych\"  0.163037 \n",
      "   12.                \"sex\"  0.162222 \n",
      "   13.    \"thyroid_surgery\"  0.160615 \n",
      "   14.           \"pregnant\"  0.159875 \n",
      "   15. \"on_antihyroid_meds\"  0.159529 \n",
      "   16.               \"sick\"  0.159110 \n",
      "   17. \"query_on_thyroxine\"  0.158888 \n",
      "   18.     \"I131_treatment\"  0.158784 \n",
      "\n",
      "Variable Importance: NUM_AS_ROOT:\n",
      "    1.                \"FTI\" 79.000000 ################\n",
      "    2.                \"TSH\" 67.000000 #############\n",
      "    3.                \"TT4\" 54.000000 ##########\n",
      "    4.                 \"T3\" 30.000000 #####\n",
      "    5. \"query_hyperthyroid\" 24.000000 ####\n",
      "    6.       \"on_thyroxine\" 14.000000 ##\n",
      "    7.  \"query_hypothyroid\" 14.000000 ##\n",
      "    8.              \"psych\"  6.000000 \n",
      "    9.                \"T4U\"  4.000000 \n",
      "   10.                \"sex\"  3.000000 \n",
      "   11.              \"tumor\"  3.000000 \n",
      "   12.                \"age\"  2.000000 \n",
      "\n",
      "Variable Importance: NUM_NODES:\n",
      "    1.                \"TSH\" 1841.000000 ################\n",
      "    2.                \"TT4\" 1532.000000 #############\n",
      "    3.                \"FTI\" 1467.000000 ############\n",
      "    4.                 \"T3\" 1455.000000 ############\n",
      "    5.                \"T4U\" 1028.000000 ########\n",
      "    6.                \"age\" 527.000000 ####\n",
      "    7.              \"tumor\" 233.000000 #\n",
      "    8.       \"on_thyroxine\" 204.000000 #\n",
      "    9. \"query_hyperthyroid\" 198.000000 #\n",
      "   10.    \"thyroid_surgery\" 87.000000 \n",
      "   11.                \"sex\" 84.000000 \n",
      "   12.  \"query_hypothyroid\" 75.000000 \n",
      "   13. \"on_antihyroid_meds\" 33.000000 \n",
      "   14.              \"psych\" 29.000000 \n",
      "   15.           \"pregnant\" 28.000000 \n",
      "   16.               \"sick\" 23.000000 \n",
      "   17. \"query_on_thyroxine\" 12.000000 \n",
      "   18.     \"I131_treatment\"  5.000000 \n",
      "\n",
      "Variable Importance: SUM_SCORE:\n",
      "    1.                \"TSH\" 374428.126350 ################\n",
      "    2.                \"FTI\" 95518.678062 ####\n",
      "    3.                \"TT4\" 75873.908137 ###\n",
      "    4.                 \"T3\" 46017.073236 #\n",
      "    5.                \"T4U\" 11484.809186 \n",
      "    6.       \"on_thyroxine\" 4005.559643 \n",
      "    7. \"query_hyperthyroid\" 3188.114177 \n",
      "    8.              \"tumor\" 2742.922550 \n",
      "    9.                \"age\" 2664.566789 \n",
      "   10.    \"thyroid_surgery\" 1192.359983 \n",
      "   11.  \"query_hypothyroid\" 997.843959 \n",
      "   12.                \"sex\" 401.923649 \n",
      "   13.              \"psych\" 294.524284 \n",
      "   14. \"on_antihyroid_meds\" 278.907895 \n",
      "   15.           \"pregnant\" 197.551074 \n",
      "   16.               \"sick\" 116.794247 \n",
      "   17. \"query_on_thyroxine\" 68.422052 \n",
      "   18.     \"I131_treatment\" 24.930841 \n",
      "\n",
      "\n",
      "Hyperparameter optimizer:\n",
      "\n",
      "Best parameters: max_depth:7\n",
      "Num steps: 4\n",
      "Best score: 0.983665\n",
      "\n",
      "Step #0 score:0.981787 parameters:{ max_depth:4 }\n",
      "Step #1 score:0.983290 parameters:{ max_depth:5 }\n",
      "Step #2 score:0.983290 parameters:{ max_depth:6 }\n",
      "Step #3 score:0.983665 parameters:{ max_depth:7 }\n",
      "\n",
      "\n",
      "Winner takes all: true\n",
      "Out-of-bag evaluation: accuracy:0.983665 logloss:0.048643\n",
      "Number of trees: 300\n",
      "Total number of nodes: 18022\n",
      "\n",
      "Number of nodes by tree:\n",
      "Count: 300 Average: 60.0733 StdDev: 7.76711\n",
      "Min: 35 Max: 85 Ignored: 0\n",
      "----------------------------------------------\n",
      "[ 35, 37)  1   0.33%   0.33%\n",
      "[ 37, 40)  1   0.33%   0.67%\n",
      "[ 40, 42)  0   0.00%   0.67%\n",
      "[ 42, 45)  4   1.33%   2.00% #\n",
      "[ 45, 47)  5   1.67%   3.67% #\n",
      "[ 47, 50) 19   6.33%  10.00% ####\n",
      "[ 50, 52) 20   6.67%  16.67% ####\n",
      "[ 52, 55) 18   6.00%  22.67% ###\n",
      "[ 55, 57) 22   7.33%  30.00% ####\n",
      "[ 57, 60) 50  16.67%  46.67% #########\n",
      "[ 60, 63) 39  13.00%  59.67% #######\n",
      "[ 63, 65) 24   8.00%  67.67% ####\n",
      "[ 65, 68) 54  18.00%  85.67% ##########\n",
      "[ 68, 70) 17   5.67%  91.33% ###\n",
      "[ 70, 73) 11   3.67%  95.00% ##\n",
      "[ 73, 75)  6   2.00%  97.00% #\n",
      "[ 75, 78)  5   1.67%  98.67% #\n",
      "[ 78, 80)  3   1.00%  99.67% #\n",
      "[ 80, 83)  0   0.00%  99.67%\n",
      "[ 83, 85]  1   0.33% 100.00%\n",
      "\n",
      "Depth by leafs:\n",
      "Count: 9161 Average: 5.31678 StdDev: 0.934646\n",
      "Min: 2 Max: 6 Ignored: 0\n",
      "----------------------------------------------\n",
      "[ 2, 3)   61   0.67%   0.67%\n",
      "[ 3, 4)  465   5.08%   5.74% #\n",
      "[ 4, 5) 1249  13.63%  19.38% ##\n",
      "[ 5, 6) 2122  23.16%  42.54% ####\n",
      "[ 6, 6] 5264  57.46% 100.00% ##########\n",
      "\n",
      "Number of training obs by leaf:\n",
      "Count: 9161 Average: 174.413 StdDev: 632.971\n",
      "Min: 5 Max: 4437 Ignored: 0\n",
      "----------------------------------------------\n",
      "[    5,  226) 8238  89.92%  89.92% ##########\n",
      "[  226,  448)  383   4.18%  94.11%\n",
      "[  448,  669)  112   1.22%  95.33%\n",
      "[  669,  891)   28   0.31%  95.63%\n",
      "[  891, 1113)   26   0.28%  95.92%\n",
      "[ 1113, 1334)   34   0.37%  96.29%\n",
      "[ 1334, 1556)   25   0.27%  96.56%\n",
      "[ 1556, 1778)   13   0.14%  96.70%\n",
      "[ 1778, 1999)    9   0.10%  96.80%\n",
      "[ 1999, 2221)   12   0.13%  96.93%\n",
      "[ 2221, 2443)    6   0.07%  97.00%\n",
      "[ 2443, 2664)   18   0.20%  97.19%\n",
      "[ 2664, 2886)   25   0.27%  97.47%\n",
      "[ 2886, 3108)   23   0.25%  97.72%\n",
      "[ 3108, 3329)   23   0.25%  97.97%\n",
      "[ 3329, 3551)   30   0.33%  98.30%\n",
      "[ 3551, 3773)   48   0.52%  98.82%\n",
      "[ 3773, 3994)   36   0.39%  99.21%\n",
      "[ 3994, 4216)   40   0.44%  99.65%\n",
      "[ 4216, 4437]   32   0.35% 100.00%\n",
      "\n",
      "Attribute in nodes:\n",
      "\t1841 : TSH [NUMERICAL]\n",
      "\t1532 : TT4 [NUMERICAL]\n",
      "\t1467 : FTI [NUMERICAL]\n",
      "\t1455 : T3 [NUMERICAL]\n",
      "\t1028 : T4U [NUMERICAL]\n",
      "\t527 : age [NUMERICAL]\n",
      "\t233 : tumor [CATEGORICAL]\n",
      "\t204 : on_thyroxine [CATEGORICAL]\n",
      "\t198 : query_hyperthyroid [CATEGORICAL]\n",
      "\t87 : thyroid_surgery [CATEGORICAL]\n",
      "\t84 : sex [CATEGORICAL]\n",
      "\t75 : query_hypothyroid [CATEGORICAL]\n",
      "\t33 : on_antihyroid_meds [CATEGORICAL]\n",
      "\t29 : psych [CATEGORICAL]\n",
      "\t28 : pregnant [CATEGORICAL]\n",
      "\t23 : sick [CATEGORICAL]\n",
      "\t12 : query_on_thyroxine [CATEGORICAL]\n",
      "\t5 : I131_treatment [CATEGORICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 0:\n",
      "\t79 : FTI [NUMERICAL]\n",
      "\t67 : TSH [NUMERICAL]\n",
      "\t54 : TT4 [NUMERICAL]\n",
      "\t30 : T3 [NUMERICAL]\n",
      "\t24 : query_hyperthyroid [CATEGORICAL]\n",
      "\t14 : query_hypothyroid [CATEGORICAL]\n",
      "\t14 : on_thyroxine [CATEGORICAL]\n",
      "\t6 : psych [CATEGORICAL]\n",
      "\t4 : T4U [NUMERICAL]\n",
      "\t3 : tumor [CATEGORICAL]\n",
      "\t3 : sex [CATEGORICAL]\n",
      "\t2 : age [NUMERICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 1:\n",
      "\t224 : TSH [NUMERICAL]\n",
      "\t199 : FTI [NUMERICAL]\n",
      "\t197 : TT4 [NUMERICAL]\n",
      "\t97 : T3 [NUMERICAL]\n",
      "\t48 : on_thyroxine [CATEGORICAL]\n",
      "\t38 : T4U [NUMERICAL]\n",
      "\t34 : query_hyperthyroid [CATEGORICAL]\n",
      "\t22 : query_hypothyroid [CATEGORICAL]\n",
      "\t14 : age [NUMERICAL]\n",
      "\t10 : psych [CATEGORICAL]\n",
      "\t8 : tumor [CATEGORICAL]\n",
      "\t6 : sex [CATEGORICAL]\n",
      "\t2 : pregnant [CATEGORICAL]\n",
      "\t1 : thyroid_surgery [CATEGORICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 2:\n",
      "\t464 : TSH [NUMERICAL]\n",
      "\t427 : FTI [NUMERICAL]\n",
      "\t419 : TT4 [NUMERICAL]\n",
      "\t278 : T3 [NUMERICAL]\n",
      "\t142 : T4U [NUMERICAL]\n",
      "\t99 : on_thyroxine [CATEGORICAL]\n",
      "\t50 : query_hyperthyroid [CATEGORICAL]\n",
      "\t40 : age [NUMERICAL]\n",
      "\t33 : tumor [CATEGORICAL]\n",
      "\t30 : query_hypothyroid [CATEGORICAL]\n",
      "\t17 : psych [CATEGORICAL]\n",
      "\t11 : thyroid_surgery [CATEGORICAL]\n",
      "\t11 : sex [CATEGORICAL]\n",
      "\t6 : pregnant [CATEGORICAL]\n",
      "\t4 : sick [CATEGORICAL]\n",
      "\t4 : on_antihyroid_meds [CATEGORICAL]\n",
      "\t3 : query_on_thyroxine [CATEGORICAL]\n",
      "\t1 : I131_treatment [CATEGORICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 3:\n",
      "\t860 : TSH [NUMERICAL]\n",
      "\t746 : FTI [NUMERICAL]\n",
      "\t726 : TT4 [NUMERICAL]\n",
      "\t554 : T3 [NUMERICAL]\n",
      "\t386 : T4U [NUMERICAL]\n",
      "\t135 : on_thyroxine [CATEGORICAL]\n",
      "\t119 : age [NUMERICAL]\n",
      "\t89 : tumor [CATEGORICAL]\n",
      "\t86 : query_hyperthyroid [CATEGORICAL]\n",
      "\t39 : query_hypothyroid [CATEGORICAL]\n",
      "\t31 : thyroid_surgery [CATEGORICAL]\n",
      "\t23 : sex [CATEGORICAL]\n",
      "\t22 : psych [CATEGORICAL]\n",
      "\t14 : on_antihyroid_meds [CATEGORICAL]\n",
      "\t9 : pregnant [CATEGORICAL]\n",
      "\t8 : sick [CATEGORICAL]\n",
      "\t4 : query_on_thyroxine [CATEGORICAL]\n",
      "\t1 : I131_treatment [CATEGORICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 5:\n",
      "\t1841 : TSH [NUMERICAL]\n",
      "\t1532 : TT4 [NUMERICAL]\n",
      "\t1467 : FTI [NUMERICAL]\n",
      "\t1455 : T3 [NUMERICAL]\n",
      "\t1028 : T4U [NUMERICAL]\n",
      "\t527 : age [NUMERICAL]\n",
      "\t233 : tumor [CATEGORICAL]\n",
      "\t204 : on_thyroxine [CATEGORICAL]\n",
      "\t198 : query_hyperthyroid [CATEGORICAL]\n",
      "\t87 : thyroid_surgery [CATEGORICAL]\n",
      "\t84 : sex [CATEGORICAL]\n",
      "\t75 : query_hypothyroid [CATEGORICAL]\n",
      "\t33 : on_antihyroid_meds [CATEGORICAL]\n",
      "\t29 : psych [CATEGORICAL]\n",
      "\t28 : pregnant [CATEGORICAL]\n",
      "\t23 : sick [CATEGORICAL]\n",
      "\t12 : query_on_thyroxine [CATEGORICAL]\n",
      "\t5 : I131_treatment [CATEGORICAL]\n",
      "\n",
      "Condition type in nodes:\n",
      "\t7850 : HigherCondition\n",
      "\t1011 : ContainsBitmapCondition\n",
      "Condition type in nodes with depth <= 0:\n",
      "\t236 : HigherCondition\n",
      "\t64 : ContainsBitmapCondition\n",
      "Condition type in nodes with depth <= 1:\n",
      "\t769 : HigherCondition\n",
      "\t131 : ContainsBitmapCondition\n",
      "Condition type in nodes with depth <= 2:\n",
      "\t1770 : HigherCondition\n",
      "\t269 : ContainsBitmapCondition\n",
      "Condition type in nodes with depth <= 3:\n",
      "\t3391 : HigherCondition\n",
      "\t461 : ContainsBitmapCondition\n",
      "Condition type in nodes with depth <= 5:\n",
      "\t7850 : HigherCondition\n",
      "\t1011 : ContainsBitmapCondition\n",
      "Node format: NOT_SET\n",
      "\n",
      "Training OOB:\n",
      "\ttrees: 1, Out-of-bag evaluation: accuracy:0.98235 logloss:0.636171\n",
      "\ttrees: 11, Out-of-bag evaluation: accuracy:0.981107 logloss:0.263055\n",
      "\ttrees: 21, Out-of-bag evaluation: accuracy:0.983099 logloss:0.174268\n",
      "\ttrees: 31, Out-of-bag evaluation: accuracy:0.98329 logloss:0.142753\n",
      "\ttrees: 41, Out-of-bag evaluation: accuracy:0.983853 logloss:0.105165\n",
      "\ttrees: 51, Out-of-bag evaluation: accuracy:0.983853 logloss:0.0868575\n",
      "\ttrees: 61, Out-of-bag evaluation: accuracy:0.984041 logloss:0.087301\n",
      "\ttrees: 71, Out-of-bag evaluation: accuracy:0.984041 logloss:0.0751795\n",
      "\ttrees: 81, Out-of-bag evaluation: accuracy:0.983853 logloss:0.0617437\n",
      "\ttrees: 91, Out-of-bag evaluation: accuracy:0.983477 logloss:0.0621338\n",
      "\ttrees: 101, Out-of-bag evaluation: accuracy:0.983477 logloss:0.0559609\n",
      "\ttrees: 111, Out-of-bag evaluation: accuracy:0.983665 logloss:0.0556003\n",
      "\ttrees: 121, Out-of-bag evaluation: accuracy:0.983477 logloss:0.05555\n",
      "\ttrees: 131, Out-of-bag evaluation: accuracy:0.983665 logloss:0.0553806\n",
      "\ttrees: 141, Out-of-bag evaluation: accuracy:0.984228 logloss:0.0554933\n",
      "\ttrees: 151, Out-of-bag evaluation: accuracy:0.983665 logloss:0.0561596\n",
      "\ttrees: 161, Out-of-bag evaluation: accuracy:0.983665 logloss:0.0558372\n",
      "\ttrees: 171, Out-of-bag evaluation: accuracy:0.983665 logloss:0.055673\n",
      "\ttrees: 181, Out-of-bag evaluation: accuracy:0.983853 logloss:0.0559052\n",
      "\ttrees: 191, Out-of-bag evaluation: accuracy:0.983853 logloss:0.0557867\n",
      "\ttrees: 201, Out-of-bag evaluation: accuracy:0.983665 logloss:0.0554635\n",
      "\ttrees: 211, Out-of-bag evaluation: accuracy:0.983477 logloss:0.0554036\n",
      "\ttrees: 221, Out-of-bag evaluation: accuracy:0.983853 logloss:0.0551438\n",
      "\ttrees: 231, Out-of-bag evaluation: accuracy:0.983853 logloss:0.0550737\n",
      "\ttrees: 241, Out-of-bag evaluation: accuracy:0.983853 logloss:0.0490497\n",
      "\ttrees: 251, Out-of-bag evaluation: accuracy:0.983665 logloss:0.0490929\n",
      "\ttrees: 261, Out-of-bag evaluation: accuracy:0.983477 logloss:0.0490362\n",
      "\ttrees: 271, Out-of-bag evaluation: accuracy:0.983477 logloss:0.0487961\n",
      "\ttrees: 281, Out-of-bag evaluation: accuracy:0.98329 logloss:0.0485993\n",
      "\ttrees: 291, Out-of-bag evaluation: accuracy:0.983665 logloss:0.0484443\n",
      "\ttrees: 300, Out-of-bag evaluation: accuracy:0.983665 logloss:0.048643\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba7d6a92d9c7295bfdcfceb88786d0d9300abf0a1802f116b1f16ab64cdf39a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
